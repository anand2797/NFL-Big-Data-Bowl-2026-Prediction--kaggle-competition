{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32d51c7",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "This module handles the downloading, extraction, and loading of data files for the NFL Game Competition project.\n",
    "## Overview \n",
    "The `data_ingestion.py` script is responsible for:\n",
    "1. Downloading a zipped dataset from a specified URL.   \n",
    "2. Unzipping the downloaded file to a designated directory.\n",
    "3. Loading the data from the unzipped files into pandas DataFrames.\n",
    "## Key Components\n",
    "- **DataIngestionConfig**: A configuration class that holds parameters for data ingestion, such as source URL, local file paths, and directories.\n",
    "- **DataIngestionArtifact**: A class that encapsulates the paths to the ingested data files.\n",
    "- **DataIngestion**: The main class that implements methods for downloading, unzipping, and loading data.\n",
    "## Usage\n",
    "1. **Initialization**: Create an instance of `DataIngestion` by passing a `DataIngestionConfig` object.\n",
    "    ```python\n",
    "    config = DataIngestionConfig(\n",
    "         source_URL=\"https://example.com/data.zip\",\n",
    "         local_data_zipped_file=\"path/to/save/data.zip\",\n",
    "         unzip_dir=\"path/to/unzip\"\n",
    "    )\n",
    "    data_ingestion = DataIngestion(config=config)\n",
    "    ```\n",
    "2. **Data Ingestion**: Call the `initiate_data_ingestion` method to perform the entire ingestion process.\n",
    "    ```python   \n",
    "    artifact = data_ingestion.initiate_data_ingestion()\n",
    "    print(artifact)\n",
    "    ```\n",
    "## Integration Steps    \n",
    "To integrate the data ingestion module into your project, follow these steps:\n",
    "**1. Define the configuration parameters in `config.yaml` and create a corresponding `DataIngestionConfig` class in `src/nfl_game_competition/entity/config_entity.py`.**\n",
    "**2. Implement the `DataIngestion` class in `src/nfl_game_competition/components/data_ingestion.py` as shown above.**\n",
    "**3. Create an `Artifact` class in `src/nfl_game_competition/entity/artifact_entity.py` to encapsulate the output of the data ingestion process.**\n",
    "**4. Update the main pipeline in `src/nfl_game_competition/pipeline/` to include the data ingestion step.**\n",
    "**5. Ensure that logging and exception handling are properly implemented for robustness.**\n",
    "## Next Steps\n",
    "After successfully ingesting the data, the next steps typically involve:\n",
    "- Data validation and schema enforcement.\n",
    "- Data preprocessing and feature engineering.\n",
    "- Model training and evaluation.\n",
    "## Note\n",
    "This module is designed to be modular and easily integrated into larger data processing pipelines. Adjust the paths and parameters as needed to fit your project's structure and requirements.  \n",
    "## How to Use This Template\n",
    "This template provides a structured approach to setting up a data science project. Follow these steps to effectively utilize the template:\n",
    "**1. Clone the repository and set up your environment.**\n",
    "- Install necessary dependencies listed in `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6103f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228af756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\kaggle\\\\nfl_game_competition\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current working directory \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f53d6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\kaggle\\\\nfl_game_competition'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go back to root directory, which : nfl_game_competition\n",
    "os.chdir(\"../\")\n",
    "# not check again \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c03ae1",
   "metadata": {},
   "source": [
    "### step 1: `src/nfl_game_competition/constants/__init__.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4d9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH=Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH= Path(\"params.yaml\")\n",
    "SCHEMA_FILE_PATH=Path(\"schema.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c427aa6c",
   "metadata": {},
   "source": [
    "### step 1.2: `src/nfl_game_competition/constants/training_pipeline_constants/__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbef680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining common constant variable for training pipeline.\n",
    "\"\"\"\n",
    "ARTIFACT_DIR: str = 'artifacts'\n",
    "PIPELINE_NAME: str = 'nfl_game_competition'\n",
    "\n",
    "TRAIN_FILE_NAME: str= \"train.csv\"\n",
    "TEST_FILE_NAME: str = \"test.csv\"\n",
    "\n",
    "SCHEMA_FILEPATH: str = os.path.join('data_schema', 'schema.yaml')\n",
    "\n",
    "SAVED_MODEL_DIR: str = os.path.join(\"models\", \"saved_models\")\n",
    "FILE_NAME: str = \"merged_data.csv\"\n",
    "\"\"\"Data Ingestion Step 1: \n",
    "This is step 1: here 'src/nfl_game_competition/constants/training_pipeline_constants/__init__.py'\n",
    "Next step 2 will be in 'src/nfl_game_competition/entity/config_entity.py' to create DataIngestionConfig class\n",
    "\n",
    "Data Ingestion related constant start with DATA_INGESTION var name.\n",
    "\"\"\"\n",
    "DATA_INGESTION_COLLECTION_NAME: str = \"NFLGameCompetitionData\"\n",
    "DATA_INGESTION_SOURCE_URL: str = \"nfl-big-data-bowl-2026-prediction\"\n",
    "DATA_INGESTION_DIR_NAME: str = \"data_ingestion\"\n",
    "DATA_INGESTION_FEATURE_STORE_DIR: str = \"feature_store\"\n",
    "DATA_INGESTION_INGESTED_DIR: str = \"ingested\"\n",
    "DATA_INGESTION_ZIPPED_FILE_DIR: str = \"zipped_data\"\n",
    "DATA_INGESTION_UNZIPPED_DIR_NAME: str = \"unzipped_data\"\n",
    "DATA_INGESTION_SPLITTED_DIR: str = \"train_test_split\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fe4c8",
   "metadata": {},
   "source": [
    "### step 2: `src/nfl_game_competition/entity/config_entity.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "#from src.nfl_game_competition.constants import training_pipeline_constants as training_pipeline\n",
    "import os\n",
    "\n",
    "# add comment for each line that tells what codeline does\n",
    "# Training Pipeline Configuration\n",
    "class TrainingPipelineConfig:\n",
    "    def __init__(self, timestamp: str = datetime.now()): \n",
    "        self.timestamp: str = timestamp.strftime(\"%d_%m_%Y_%H_%M_%S\")   # format timestamp,  set timestamp\n",
    "        self.pipeline_name = PIPELINE_NAME              # set pipeline name\n",
    "        self.artifact_name = ARTIFACT_DIR              # set artifact name\n",
    "        self.artifact_dir = os.path.join(self.artifact_name, self.timestamp)   # set artifact directory\n",
    "        self.model_dir = os.path.join(self.artifact_dir, \"final_model\")  # set model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e16b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        self.data_ingestion_dir: str = os.path.join(training_pipeline_config.artifact_dir,\n",
    "                                                    DATA_INGESTION_DIR_NAME)  # data ingestion directory  \n",
    "        # feature store file path as(artifacts/data_ingestion/feature_store/nfl_game_features.csv)\n",
    "        self.feature_store_filepath: str = os.path.join(self.data_ingestion_dir, \n",
    "                                                        DATA_INGESTION_FEATURE_STORE_DIR,\n",
    "                                                        FILE_NAME)  # feature store file path\n",
    "        # training file path as(artifacts/data_ingestion/ingested/train_test_split/train.csv)\n",
    "        self.training_filepath: str = os.path.join(self.data_ingestion_dir, \n",
    "                                                   DATA_INGESTION_INGESTED_DIR,\n",
    "                                                   DATA_INGESTION_SPLITTED_DIR,\n",
    "                                                   TRAIN_FILE_NAME) # training file path \n",
    "        self.testing_filepath: str = os.path.join(self.data_ingestion_dir, \n",
    "                                                  DATA_INGESTION_INGESTED_DIR,\n",
    "                                                  DATA_INGESTION_SPLITTED_DIR,\n",
    "                                                  TEST_FILE_NAME)  # test filepath\n",
    "        # create variable for source url\n",
    "        self.data_source_url: str = DATA_INGESTION_SOURCE_URL  # data source url\n",
    "        # create variable for collection name\n",
    "        self.data_collection_name: str = DATA_INGESTION_COLLECTION_NAME  # data collection\n",
    "\n",
    "        # downloaded data zip file path\n",
    "        # artifacts/data_ingestion/ingested/data.zip\n",
    "        self.zipped_data_filepath: str = os.path.join(self.data_ingestion_dir,\n",
    "                                                      DATA_INGESTION_INGESTED_DIR,\n",
    "                                                      DATA_INGESTION_ZIPPED_FILE_DIR)  # zipped data file path\n",
    "        \n",
    "        # unzipped data directory path\n",
    "        # artifacts/data_ingestion/ingested/unzipped_data\n",
    "        self.unzipped_data_dir: str = os.path.join(self.data_ingestion_dir,\n",
    "                                                   DATA_INGESTION_INGESTED_DIR,\n",
    "                                                   DATA_INGESTION_UNZIPPED_DIR_NAME)  # unzipped data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66debca0",
   "metadata": {},
   "source": [
    "### step 2.1: `src/nfl_game_competition/entity/artifact_entity.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be46f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    feature_store_file_path: str\n",
    "    train_file_path: str\n",
    "    test_file_path: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c96eb",
   "metadata": {},
   "source": [
    "### step 3: `src/nfl_game_competition/component/data_ingestion.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 now , data ingestion component\n",
    "\"\"\"\n",
    "# we have to create data ingestion class in which there multiple fuction , \n",
    "# 1) get data from kaggleapi based on provided competition name as source_url mentioned in DataIngestionConfig class, and save it into \n",
    "zipped_data_filepath \n",
    "2) unzip the data and print all file names (which we can use their pattern in glob to concat input or woutpur wise) and save it into unzipped_data_dir\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "#from src.nfl_game_competition.entity.config_entity import DataIngestionConfig, TrainingPipelineConfig\n",
    "from src.nfl_game_competition.exception import NFLGameCompetitionException\n",
    "from src.nfl_game_competition.logger import get_logger\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "# import utils fuction to create directory \n",
    "from src.nfl_game_competition.utils.common import create_directories\n",
    "\n",
    "logger = get_logger(name=\"1_data_ingestion_notebook\")\n",
    "\n",
    "# we have to create data ingestion class in which there multiple fuction , \n",
    "# 1) get data from kaggleapi based on provided competition name as source_url mentioned in DataIngestionConfig class, and save it into \n",
    "# zipped_data_filepath  \n",
    "#2) unzip the data and save it into unzipped_data_dir\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, data_ingestion_config: DataIngestionConfig):\n",
    "        try:\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "            self.kaggle_api = KaggleApi()\n",
    "            self.kaggle_api.authenticate()\n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys) from e\n",
    "        \n",
    "    \n",
    "    def download_nfl_data(self) -> str:\n",
    "        \"\"\"\n",
    "        Downloads the NFL data from Kaggle using the Kaggle API and saves it to the specified zipped data filepath.\n",
    "        Returns the path to the downloaded zip file.\n",
    "        use competition_download_files method of KaggleApi class\n",
    "        1) competition: str - The name of the competition to download data from.\n",
    "        2) path: str - The directory where the downloaded files will be saved.\n",
    "        use create_dirctory function to create directory if not exist\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting data download from kaggle\")\n",
    "            # create directory if not exist\n",
    "            #print(self.data_ingestion_config.zipped_data_filepath)\n",
    "\n",
    "            create_directories([self.data_ingestion_config.zipped_data_filepath])\n",
    "            # download the data from kaggle\n",
    "            self.kaggle_api.competition_download_files(competition=self.data_ingestion_config.data_source_url,\n",
    "                                                       path=self.data_ingestion_config.zipped_data_filepath)\n",
    "                                                       \n",
    "            logger.info(f\"Data downloaded successfully from kaggle and saved to {self.data_ingestion_config.zipped_data_filepath}\")\n",
    "            return self.data_ingestion_config.zipped_data_filepath\n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys) from e\n",
    "        \n",
    "    # unzip downloaded data and store to unzipped_data_dir , fuction return unzipped_data_dir path\n",
    "    def unzip_and_extract_data(self, zip_file_path: str, extract_dir: str) -> Path:\n",
    "        \"\"\"\n",
    "        Unzips the downloaded NFL data zip file and extracts its contents to the specified directory.\n",
    "        Returns the path to the directory where the files were extracted.\n",
    "        Args:\n",
    "            1) zip_file_path: str - The path to the zip file to be extracted.\n",
    "            2) extract_dir: str - The directory where the extracted files will be saved.\n",
    "        use create_dirctory function to create directory if not exist\n",
    "        return:\n",
    "            pathlib.Path: The path to the directory where the files were extracted.\n",
    "            also return two strings where one str tells pattern for training input data, another str tells pattern for target/output data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting to unzip and extract data from {zip_file_path} to {extract_dir}\")\n",
    "            # create directory if not exist\n",
    "            create_directories([extract_dir])\n",
    "            # unzip the data\n",
    "            # find zip file name in zip_file_path\n",
    "\n",
    "            print(\"zip file name: \", self.data_ingestion_config.data_source_url)\n",
    "            with ZipFile(f\"{zip_file_path}/{self.data_ingestion_config.data_source_url}.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "            logger.info(f\"Data unzipped and extracted successfully to {extract_dir}\")\n",
    "            return Path(extract_dir)\n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys) from e\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        try:\n",
    "            logger.info(f\"{'>>'*20} Data Ingestion {'<<'*20}\")\n",
    "            zip_file_path = self.download_nfl_data()\n",
    "            unzip_dir = self.unzip_and_extract_data(zip_file_path, self.data_ingestion_config.unzipped_data_dir)\n",
    "            logger.info(f\"Data Ingestion - data downloded done: {unzip_dir}\")\n",
    "            #full_df = self.load_data_multiple_files(unzip_dir)\n",
    "            #train_set, test_set = self.train_test_splitting(full_df)\n",
    "            #train_filepath = self.export_data_into_feature_store(train_set, \"train\")\n",
    "            #test_filepath = self.export_data_into_feature_store(test_set, \"test\")\n",
    "            #data_ingestion_aftifacts = DataIngestionArtifact(trained_filepath=train_filepath,\n",
    "                                                             #test_filepath=test_filepath)\n",
    "            #logger.info(f\"Data Ingestion Completed. Artifacts: {data_ingestion_aftifacts}\")\n",
    "            return unzip_dir\n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bea1f",
   "metadata": {},
   "source": [
    "### step 4: `src/nfl_game_competition/pipelines/train_pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb189f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nfl_game_competition.entity.artifact_entity import DataIngestionArtifact \n",
    "#from src.nfl_game_competition.entity.config_entity import (TrainingPipelineConfig, DataIngestionConfig) \n",
    "#from src.nfl_game_competition.components.data_ingestion import DataIngestion\n",
    "\n",
    "\n",
    "logger = get_logger(name=\"nfl_game_competition/pipelines/train_pipeline.py\")\n",
    "class TrainingPipeline:\n",
    "    def __init__(self):\n",
    "        self.training_pipeline_config = TrainingPipelineConfig()\n",
    "\n",
    "    def start_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            self.data_ingestion_config = DataIngestionConfig(training_pipeline_config=self.training_pipeline_config)\n",
    "            data_ingestion = DataIngestion(config=self.data_ingestion_config)\n",
    "            unzip_dir = data_ingestion.initiate_data_ingestion()\n",
    "            #logger.info(f\"Data Ingestion Artifact: {data_ingestion_artifact}\")\n",
    "            return unzip_dir\n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys)\n",
    "\n",
    "    # run pipeline\n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_data_ingestion()\n",
    "            # data_validation_artifact = self.start_data_validation(data_ingestion_artifact=data_ingestion_artifact)\n",
    "            # data_transformation_artifact = self.start_data_transformation(data_validation_artifact=data_validation_artifact)\n",
    "            # model_trainer_artifact = self.start_model_training(data_transformation_artifact=data_transformation_artifact)\n",
    "            # model_evaluation_artifact = self.start_model_evaluation(data_validation_artifact=data_validation_artifact,\n",
    "            #                                                        model_trainer_artifact=model_trainer_artifact)\n",
    "            # model_pusher_artifact = self.start_model_pusher(model_evaluation_artifact=model_evaluation_artifact)\n",
    "            # return model_pusher_artifact\n",
    "            return data_ingestion_artifact\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise NFLGameCompetitionException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ad56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the pipeline is running\n",
    "if __name__ == \"__main__\":\n",
    "    train_pipeline = TrainingPipeline()\n",
    "    train_pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba3e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b448f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a7aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ca0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9f905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66973a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11751553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "893eea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts\\10_15_2025_16_09_13\\data_ingestion\\ingested\\zipped_data\n",
      "Downloaded zip file path: artifacts\\10_15_2025_16_09_13\\data_ingestion\\ingested\\zipped_data\n",
      "zip file name:  nfl-big-data-bowl-2026-prediction\n",
      "List of all files in the extracted directory:\n"
     ]
    }
   ],
   "source": [
    "# check if everything is fine\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        training_pipeline_config = TrainingPipelineConfig()\n",
    "        data_ingestion_config = DataIngestionConfig(training_pipeline_config=training_pipeline_config)\n",
    "        #print(data_ingestion_config.__dict__)\n",
    "\n",
    "        data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "        zip_file_path = data_ingestion.download_nfl_data()\n",
    "        print(f\"Downloaded zip file path: {zip_file_path}\")\n",
    "        \n",
    "        extracted_path = data_ingestion.unzip_and_extract_data(zip_file_path=zip_file_path,\n",
    "                                                              extract_dir=data_ingestion_config.unzipped_data_dir)\n",
    "        #print(f\"Extracted data directory: {extracted_path}\")\n",
    "\n",
    "        # List all files in the extracted directory\n",
    "        #all_files = glob.glob(os.path.join(extracted_path, '*.csv'), recursive=True)\n",
    "        print(\"List of all files in the extracted directory:\")\n",
    "        #for file in all_files:\n",
    "           # print(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise NFLGameCompetitionException(e, sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca57eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete! Files are saved in: data_sample\n"
     ]
    }
   ],
   "source": [
    "# check data downloading by kaggle api\n",
    "\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi  # Import Kaggle API\n",
    "\n",
    "# Step 1: Authenticate using kaggle.json file (must be in ~/.kaggle)\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # This reads your ~/.kaggle/kaggle.json file for username & key\n",
    "\n",
    "# Step 2: Define the competition name\n",
    "competition_name = \"nfl-big-data-bowl-2026-prediction\"  # replace with your actual competition name\n",
    "\n",
    "# Step 3: Define a local directory to save the data\n",
    "download_dir = \"data_sample\"  # folder where data will be downloaded\n",
    "os.makedirs(download_dir, exist_ok=True)  # create folder if not exists\n",
    "\n",
    "# Step 4: Download all competition files as ZIP\n",
    "api.competition_download_files(competition_name, path=download_dir)  # downloads and saves zip file\n",
    "\n",
    "print(f\"✅ Download complete! Files are saved in: {download_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
